[TOC]

[é‡è¦è®ºæ–‡çš„æ€»ç»“](https://docs.google.com/presentation/d/13k5cs4p_OmMKkNB9YVuF2CkzZ3PTwUV5bcAIHL4sqjk/edit?usp=sharing)



- [Papers](#Papers)



# Papers

## Base Models

1. `LLaMA: Open and Efficient Foundation Language Models.` arxiv 2023
   Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. [[pdf]](https://arxiv.org/abs/2302.13971v1)

2. `Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling` 2023
   Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal [[paper](https://arxiv.org/abs/2304.01373)] [[project](https://github.com/EleutherAI/pythia)]

   Model Size: 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, 12B

3. `Opt-iml: Scaling language model instruction meta learning through the lens of generalization.` arxiv 202
   Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Da Ìniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. [[paper](https://arxiv.org/abs/2212.12017)]
4. [ChatGLM: An Open Bilingual Dialogue Language Model](https://github.com/THUDM/ChatGLM-6B)
5. [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
6. [FLAN: Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
7. [BELLE: Bloom-Enhanced Large Language model Engine](https://github.com/LianjiaTech/BELLE)
8. [GPT4All: Training an Assistant-style Chatbot with Large Scale Data Distillation from GPT-3.5-Turbo](https://github.com/nomic-ai/gpt4all)



## Method to Get Training Data

1. `Self-Instruct: Aligning Language Model with Self Generated Instructions` arXiv 2022
   *Wang, Yizhong , Kordi, Yeganeh , Mishra, Swaroop , Liu, Alisa , Smith, Noah A. , Khashabi, Daniel , Hajishirzi, Hannaneh*  [[pdf](https://arxiv.org/pdf/2212.10560.pdf)] [[project](https://github.com/yizhongw/self-instruct)]
2. 



## å®ç°ChatGPTçš„å¹³æ›¿

> ä¸»è¦è·ŸInstruction Tuningæœ‰å…³ï¼Œå³æ„å»º Instruction + Nä¸ªInput-output examplesã€‚
> Alpaca, Vicuna, Dollyéƒ½æ²¡æœ‰ä½¿ç”¨RLHF

1. `Alpaca: A Strong, Replicable Instruction-Following Model` 2023
   Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto  [[blog](https://crfm.stanford.edu/2023/03/13/alpaca.html)] [[project](https://github.com/tatsu-lab/stanford_alpaca)]
   åŸºäºLLaMaè®­ç»ƒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®æ¥è‡ªäºtext-davinci-003çš„æ ‡æ³¨

2. `Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality` 2023
   Chiang, Wei-Lin, Li, Zhuohan, Lin, Zi, Sheng, Ying, Wu, Zhanghao, Zhang, Hao, Zheng, Lianmin, Zhuang, Siyuan, Zhuang, Yonghao, Gonzalez, Joseph E., Stoica, Ion, Xing, Eric P. [[blog](https://vicuna.lmsys.org/)] [[project](https://github.com/lm-sys/FastChat)]
   åŸºäºLLaMaè®­ç»ƒçš„æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®æ¥è‡ªäºchatgptçš„æ ‡æ³¨(share gpt)

3. `Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM`
   Mike Conover, Matt Hayes, Ankit Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, Reynold Xin [[blog](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)] [[project](https://github.com/databrickslabs/dolly)]

   åŸºäºPythia 12bï¼Œè®­ç»ƒæ•°æ®æ¥è‡ªäºDataBrickså‘˜å·¥çš„æ ‡æ³¨ `databricks-dolly-15k`

4. 



## PEFT (Parameter Efficient Fine-Tuning)

1. ![LoRA](https://img.shields.io/badge/LoRA-blue)
     `LoRA: Low-Rank Adaptation of Large Language Models` 2021
       Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen [[pdf](https://arxiv.org/abs/2106.09685)]
     æ¨¡å‹ï¼šRoBERTa, DeBERTaï¼Œ ä»»åŠ¡ï¼šGLUE
     æ¨¡å‹ï¼šGPT2 M/Lï¼Œ ä»»åŠ¡ï¼šE2E NLG, WebNLG, DART
     æ¨¡å‹ï¼šGPT3 175Bï¼Œä»»åŠ¡ï¼šWikiSQL, MNLI-m, SAMSum
     åšäº†Low-data Settingçš„æƒ…å†µ

2. ![AdaLoRA](https://img.shields.io/badge/AdaLoRA-blue) 

     `Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning` ICLR2023
     Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao [[pdf](https://arxiv.org/abs/2303.10512)] ==TODO==

3. ![Prefix Tuning](https://img.shields.io/badge/PrefixTuning-blue) 

   `Prefix-Tuning: Optimizing Continuous Prompts for Generation` ACL 2021 
   Xiang Lisa Li, Percy Liang [[pdf](https://aclanthology.org/2021.acl-long.353/)]
   æ¨¡å‹ï¼šGPT2 M/L, BART Lï¼Œä»»åŠ¡ï¼šE2E, WebNLG, DART, XSUM (Summarization)
   **æ¯ä¸€å±‚**éƒ½åŠ äº†è‹¥å¹²ä¸ªhidden statesä½œä¸ºå‚æ•°ã€‚

   `P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks` ACL 2022                         
   Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang [[pdf](https://arxiv.org/abs/2110.07602)]
   **æ¯ä¸€å±‚**éƒ½åŠ äº†è‹¥å¹²ä¸ªhidden statesä½œä¸ºå‚æ•°ã€‚
   æ¨¡å‹ï¼šBERT (335M), RoBERTa (355M), GLM (2B,10B)ï¼Œä»»åŠ¡ï¼šSuperGLUE
   æ¨¡å‹ï¼šBERT, RoBERTa, DeBERTa (750M)ï¼Œä»»åŠ¡ï¼š

   - **NER**: CoNLL03, OntoNotes 5.0, CoNLL04
   - **Extractive QA**: SQuAD 1.1, SQuAD 2.0
   - **SRL (Semantic Role Labeling)**: CoNLL12, CoNLL05 WSJ, CoNLL05 Brown

4. ![P-Tuning](https://img.shields.io/badge/PTuning-blue) 

   `GPT Understands, Too`  arxiv 2021
   Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang [[pdf](https://arxiv.org/abs/2103.10385)]
   æ¨¡å‹ï¼šBERT, RoBERTa, GHT2-medium, GPT2-xl, MegatronLMï¼Œä»»åŠ¡ï¼šLAMA, SuperGLUE
   åœ¨**Word Embedding**å±‚é¢ä½¿ç”¨Coninuous Promptsï¼Œç¦»æ•£tokenå’Œè¿ç»­æç¤ºè¿›è¡Œäº†æ··åˆã€‚

5. ![Prompt Tuning](https://img.shields.io/badge/PromptTuning-blue) 

   `The Power of Scale for Parameter-Efficient Prompt Tuning` EMNLP2021
   Brian Lester, Rami Al-Rfou, Noah Constant [[pdf](https://arxiv.org/abs/2104.08691)]
   æ¨¡å‹ï¼šT5ï¼Œä»»åŠ¡ï¼šSuperGLUE
   åœ¨**Word Embedding**å±‚é¢ä½¿ç”¨Coninuous Promptsï¼Œè€ƒè™‘äº†åŒæ—¶è®­ç»ƒå¤šä¸ªä»»åŠ¡çš„æƒ…å†µ

6. ![PPT](https://img.shields.io/badge/PPT-blue)
     `PPT: Pre-trained Prompt Tuning for Few-shot Learning`  ACL 2022
     Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang [[pdf](https://arxiv.org/abs/2109.04332)]
     å¯ä»¥çœ‹æˆ Pretraining + Prompt Tuning
     æ¨¡å‹ T5-XXL (11B)ï¼ŒmT5-XXLï¼ŒCPM-2ï¼Œä»»åŠ¡ï¼šä¸»è¦é’ˆå¯¹å¤šé€‰é¡¹åˆ†ç±»(Multiple-Choice Classification)

     | è¯­è¨€ | ç±»åˆ«                      | æ•°æ®é›†                 |
     | ---- | ------------------------- | ---------------------- |
     | è‹±è¯­ | å•å¥è¯åˆ†ç±»                | SST-2, SST-5, YahooAns |
     |      | å¤šé€‰åˆ†ç±»                  | RACE-m, RACE-h         |
     |      | å¥å­é—´åˆ†ç±»(Sentence Pair) | BoolQ, RTE, CB         |
     | ä¸­æ–‡ | å•å¥è¯åˆ†ç±»                | ChnSent, Amazon, TNews |
     |      | å¤šé€‰åˆ†ç±»                  | CCPM, C3               |
     |      | å¥å­é—´åˆ†ç±»(Sentence Pair) | LCQMC, CMNLI, OCNLI    |

7. ![Adapterå®¶æ—](https://img.shields.io/badge/Adapterå®¶æ—-blue)åˆ—è¡¨ï¼š
	
	  - ![AdapterH](https://img.shields.io/badge/AdapterH-blue) (æœ€åŸå§‹çš„Adapter)
	    `Parameter-Efficient Transfer Learning for NLP` ICML 2019
	    Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly [[pdf](https://arxiv.org/abs/1902.00751)]
	    æ¨¡å‹ï¼šBERTï¼Œä»»åŠ¡ï¼šGLUEï¼Œ17ä¸ªå…¶ä»–çš„classification taskså’ŒSQuAD
	  - ![AdapterL](https://img.shields.io/badge/AdapterL-blue)
	    `Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning` EMNLP 2020
	    Zhaojiang Lin, Andrea Madotto, Pascale Fung [[pdf](https://aclanthology.org/2020.findings-emnlp.41/)]
	    é™¤äº†è®­ç»ƒAdapterï¼Œè¿˜è®­ç»ƒå¯¹åº”çš„Task Embeddingç”¨äºåŠ åœ¨è¾“å…¥ä¸Šã€‚
	    æ¨¡å‹ï¼šGPT2-smallï¼Œä»»åŠ¡ï¼š
	    - Chit-chat based Dialogue(**DLG**): PersonaChat
	
	    - **NMT**: IWSLT German-English
	
	    - Summarization (**SUM**): CNN/Daily-Mail
	
	    - Conversational QA (**CQA**): CoQA
	
	    - (**NLG**): E2E NLG-Challenge(2019)
	
	  - ![AdapterP](https://img.shields.io/badge/AdapterP-blue) 
	    `AdapterFusion: Non-Destructive Task Composition for Transfer Learning` EACL 2021
	    Jonas Pfeiffer, Aishwarya Kamath, Andreas RÃ¼cklÃ©, Kyunghyun Cho, Iryna Gurevych [[pdf](https://aclanthology.org/2021.eacl-main.39.pdf)]
	    æ¨¡å‹ï¼šBERT-base-uncasedï¼Œä»»åŠ¡ï¼š
	    - **Commonsense Reasoning**: Hellaswag, Winogrande, CosmosQA, CSQA, SocialQA
	    - **Sentiment Analysis**: IMDb, SST
	    - **Natural Language Inference**:MNLI, RTE, CB, SciTail, SICK
	    - **Sentence Relatedness**: MRPC, QQP, Arugment, BoolQ
	
	  - ![AdapterD](https://img.shields.io/badge/AdapterD-blue)
	    `AdapterDrop: On the Efficiency of Adapters in Transformers` EMNLP2021
	    Andreas RÃ¼cklÃ©, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, Iryna Gurevych [[pdf](https://arxiv.org/abs/2010.11918)]
	



PEFTç»Ÿä¸€æ¡†æ¶

1. ` Towards a Unified View of Parameter-Efficient Transfer Learning`
2. `UNIPELT: A Unified Framework for Parameter-Efficient Language Model Tuning`
3. `Revisiting Parameter-Efficient Tuning: Are We Really There Yet?`
4. `Sparse Structure Search for Parameter-Efficient Tuning`
5. `Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models`



## Plug-And-Play



## Evaluating LLMs

1. `Instruction Tuning with GPT-4` arxiv 2023
   Peng, Baolin, Li Chunyuan , He Pengcheng , Galley Michel , Gao Jianfeng [[blog](https://instruction-tuning-with-gpt-4.github.io/)] [[paper](https://arxiv.org/pdf/2304.03277.pdf)] [[project](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)]
   
   åŸºäºLLaMA 7Bè¿›è¡ŒSFTï¼ŒåŸºäºOPT 1.3Bè®­ç»ƒå¾—åˆ°Reward Modelã€‚
   
   - Instructionæ•ˆæœçš„è¯„ä¼°ï¼Œä½¿ç”¨GPT-4ç”ŸæˆInstruction-following data
   - 3ç§è¯„ä¼°æŒ‡æ ‡ï¼š
     - human evaluation on three alignment criteria: **Helpfulness** (æ¯”å¦‚è¶Šæ­£ç¡®ã€è¶Šç›¸å…³ï¼Œç­”æ¡ˆå¯èƒ½å°±è¶Šæœ‰å¸®åŠ©), **Honesty**ï¼‰ï¼ˆæ˜¯å¦æœ‰è™šå‡ä¿¡æ¯ï¼‰, **Harmlessness**ï¼ˆæ˜¯å¦æœ‰ä»‡æ¨ã€æš´åŠ›çš„å†…å®¹ï¼‰
     - automatic evaluation using GPT-4 feedback,  [è®©GPT-4ç»™ç»“æœä»1-10æ‰“åˆ†]
     - **ROUGE-L** on un-natural instructions
   
2.  `Holistic Evaluation of Language Models` arxiv 2023
   Liang P, Bommasani R, Lee T, et al. [[paper](https://arxiv.org/pdf/2211.09110.pdf)] [[project](https://github.com/stanford-crfm/helm)]

3. `Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models` arxiv 2023
   Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. [[paper](https://arxiv.org/abs/2206.04615)] [[project](https://github.com/google/BIG-bench)]

   The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. Big-bench include more than 200 tasks.

4. `PandaLM: Reproducible and Automated Language Model Assessment` GitHub 2023
   Wang Yidong, Yu Zhuohao, Zeng Zhengran, Yang Linyi, Heng Qiang, Wang Cunxiang, Chen Hao, Jiang Chaoya, Xie Rui, Wang Jindong, Xie Xing, Ye Wei, Zhang Shikun and Zhang Yue. [[project](https://github.com/WeOpenML/PandaLM)]


## LLMå¸¦æ¥çš„æ–°æ–¹å‘



## Model Explanation

> åœ¨æ¨¡å‹ä¸­åŠ å…¥å¯è§£é‡Šæ€§æ¨¡å—



## å¤§æ¨¡å‹è®­ç»ƒçš„æŠ€å·§

ç¡¬ä»¶å±‚é¢

-  **Flash Attention** (Dao et al., 2022)

æ¨¡å‹å±‚é¢

- **rotary embeddings**  (Su et al. 2021)

- **parallelized attention and feedforward technique** (Wang & Komatsuzaki 2021)
- **untied embedding / unembedding matrices** (Belrose et al., 2023)

## Applications of LLMs
- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)
- `HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace` arxiv 2023
   Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang. [[paper](https://arxiv.org/pdf/2303.17580.pdf)] [[project](https://github.com/microsoft/JARVIS)]



# Tutorials



# Resources

- [LLM Zoo](https://github.com/FreedomIntelligence/LLMZoo): 
- [ChatPiXiu](https://github.com/catqaq/ChatPiXiu): ChatGPTå¼€æºå¹³æ›¿åŠé¢†åŸŸé€‚åº”

## Data

- [InstructionZoo](https://github.com/FreedomIntelligence/InstructionZoo): å„ç§Instruction-Tuningçš„æ•°æ®é›†

- [awesome-chatgpt-dataset](https://github.com/voidful/awesome-chatgpt-dataset/): å„ç§Instruction-Tuningçš„æ•°æ®é›†

- [Alpaca Training Data](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)

- [Vicuna Training Data: ShareGPT](https://github.com/lm-sys/FastChat/issues/90)

- Dolly 2.0 Training Data: [databricks-dolly-15k](https://github.com/databrickslabs/dolly/tree/master/data)



## Online Demo

- [Vicuna, Koala, FastChat-T5, OpenAssistant, ChatGLM, StableLM, Alpaca, LLaMa, Dolly](https://chat.lmsys.org/)



## Models





## Tools

- [[peft](https://github.com/huggingface/peft)] from HuggingfaceğŸ¤—: å®ç°äº†LoRA, Prefix Tuning, P-Tuning, Prompt Tuningå’ŒAdaLoRAã€‚
- 



# å‚è€ƒ
[ã€OpenLLM 007ã€‘å°å‚æ•°æ’¬åŠ¨å¤§æ¨¡å‹-ä¸‡å­—é•¿æ–‡å…¨é¢è§£è¯»PEFTå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯](https://zhuanlan.zhihu.com/p/625502729)



# TODO

Decoupling Knowledge From Memorization Retrieval-augmented Prompt Learning

[ç¥å¥‡LLMå¼•æ“ä¸Šçº¿ï¼šå¸®ä½ æŠŠGPT-3ç›´æ¥è°ƒæˆChatGPT](https://mp.weixin.qq.com/s/eBFjLfyLycdMIF6-ucgy1w)

[æ¸…åå”æ°æ•™æˆï¼šä»åƒäº¿æ¨¡å‹åˆ°ChatGPTçš„â¼€ç‚¹æ€è€ƒ](https://mp.weixin.qq.com/s/25cxLdYd37DHw6-UpZlayw)



[Prompt-Tuningâ€”â€”æ·±åº¦è§£è¯»ä¸€ç§æ–°çš„å¾®è°ƒèŒƒå¼](https://zhuanlan.zhihu.com/p/619566088)

# Else

`Language Models as Knowledge Bases?` EMNLP 2019.

*Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel.* 2019.9 [[pdf](https://arxiv.org/abs/1909.01066)] [[project](https://arxiv.org/abs/1909.01066)]

![](https://img.shields.io/badge/T5-blue) The abbreviation of the work.

![](https://img.shields.io/badge/Continuous_Template-red) The key features in terms of prompt learning used in the work.

![](https://img.shields.io/badge/Generation-brown) The mainly explored task of the work.

![](https://img.shields.io/badge/Analysis-green) The mainly explored property of prompt learning methods in the work.