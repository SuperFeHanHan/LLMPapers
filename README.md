[TOC]

[重要论文的总结](https://docs.google.com/presentation/d/13k5cs4p_OmMKkNB9YVuF2CkzZ3PTwUV5bcAIHL4sqjk/edit?usp=sharing)

# Papers

## PEFT (Parameter Efficient Fine-Tuning)



## Plug-And-Play



## 如何评估LLMs



## LLM带来的新方向



## Model Explanation

> 在模型中加入可解释性模块



# Blogs



# Videos



# Tutorials

如何实现ChatGPT的平替



# Resources



## Data



## Models




# TODO

Decoupling Knowledge From Memorization Retrieval-augmented Prompt Learning

[神奇LLM引擎上线：帮你把GPT-3直接调成ChatGPT](https://mp.weixin.qq.com/s/eBFjLfyLycdMIF6-ucgy1w)

[清华唐杰教授：从千亿模型到ChatGPT的⼀点思考](https://mp.weixin.qq.com/s/25cxLdYd37DHw6-UpZlayw)

# Else

`Language Models as Knowledge Bases?` EMNLP 2019.

*Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller, Sebastian Riedel.* 2019.9 [[pdf](https://arxiv.org/abs/1909.01066)] [[project](https://arxiv.org/abs/1909.01066)]